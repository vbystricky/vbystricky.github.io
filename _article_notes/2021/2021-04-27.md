---
layout: article_notes
title: Изображения как набор слов 16 х 16.
date: 2021-04-27
tags: [CNN, attention]
cite: "arXiv:2010.11929"
link: https://arxiv.org/abs/2010.11929
use_math: true
published: true
send2tg: true
---
[{{ page.cite }}]({{ page.link }})

Если [здесь]({% link _article_notes/2021/2021-04-20.md%}) attention модель применялась "локально" и фактически self-attention преобразование служило
заменой стандартной свёртки, то сегодня посмотрим на статью, в которой авторы попытались максимально перенести подход, предложенный для решения задачи
перевода в компьютерное зрение.

<!--more-->

На самом деле, первым делом есть смысл разобраться [с исходной статьёй](https://arxiv.org/abs/1706.03762) об *attention* модели. Для этого
порекомендую [чудесный разбор](http://jalammar.github.io/illustrated-transformer/) от нашего зарубежного товарища, с картиночками и крайне подробный.
Когда с базовой концепцией разобрались, новую статью можно понять за одну картинку:

![схема, картинка из статьи]({{ site.baseurl }}/images/article_notes/2021/arxiv.2010.11929_1.png)

Т.е. модель полностью повторяет transformer применяемый для переводов текстов. Только вместо слов из предложения, которое хотим перевести, (слова у
нас не просто кодированы one-hot по словарю, а вначале вложены в некоторое векторное пространство при помощи word2vec преобразования) на вход
подаются патчи на которые режим картинку. Причем, патчи тоже не плохо вначале прогнать через какую-нибудь сверточную сетку типа ResNet и
соответственно в transformer отдавать уже не сами пиксели, а наборы откликов.

Судя по статистике, авторы обещают повышение качества на 1-1.5%, но зато снижение времени тренировки до 4-х раз.

