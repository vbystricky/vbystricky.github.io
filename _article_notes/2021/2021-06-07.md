---
layout: article_notes
title: Управляемый линейный модуль (Gated Linear Unit).
date: 2021-06-07
tags: [CNN, RNN, GLU]
cite: "arXiv:1612.08083"
link: https://arxiv.org/abs/1612.08083
use_math: true
published: true
send2tg: true
---
[{{ page.cite }}]({{ page.link }})

На самом статья про построение модели языка с использованием управляемых свёрточных сетей (*Gated Convolutional Networks*), но мне область применения
не очень интересна (плюс, думаю в этой области такой подход уже несколько устарел, там сейчас рулит *BERT* и вот это вот всё), а вот базовый блок,
предложенный в данной статье хочется разобрать, чтобы потом можно было ссылаться. Потому что этот блок может быть использован при работе с временными
последовательностями в качестве замены рекуррентных сетей.

<!--more-->

Пусть имеется некая временная последовательность векторов $x_0, ..., x_N$ (в исходной статье это слова предложения, но это не существенно). Мы хотим
получить представление данной последовательности: $H = [h_0, ..., h_N]$, которое позволит по $h_i$ предсказывать $x_{i+1}$. Обычно для этого
используются [рекуррентые нейронные сети]({% post_url 2021-05-10-rnn_lstm_gru_etc%}), но авторы статьи предлагают следующую замену.

![схема GLU]({{ site.baseurl }}/images/article_notes/2021/arxiv.1612.08083_1.svg)

Итак, у нас есть входной набор из $N$ векторов $X \in \mathbb R^{N\times m}$, мы используем преобразование:

$$
H \equiv h(X) = \left(X \ast W + b\right) \odot \sigma \left(X \ast V + c\right)
$$

здесь $\ast$ обозначает операцию свёртки, $W, V \in \mathbb R^{k\times m \times n}$ - свёрточные ядра, с шириной $k$ и количеством выходных каналов
$n$, $\sigma$ - нелинейность (в исходной статье предлагается сигмоид), а $\odot$ - поэлементное умножение.

Такой блок авторы называют *Gated Linear Unit (GLU)*. Не трудно заметить, что концепция *ворот* взята из LSTM ячеек и так же как и в LSTM позволяет
бороться с проблемой исчезающих градиентов при тренировке.

Исходные данные дополняются $k-1$-м нулевым вектором в начале последовательности, чтобы результат на позиции $i$ вычислялся с использованием только
векторов $x_j, \, j < i$, т.е. не заглядывал в будущее. При этом за счёт использования свёртки в $i$-ый выходной вектор попадает информация из $k - 1$
предыдущих по времени векторов.

Аналогично, как мы это делаем для других сетей, мы можем и здесь собрать сеть из нескольких слоёв.

На этом всё. В статье авторы описывают эксперименты и дают цифры сравнения для нового блока и LSTM, если интересно, можно ознакомиться подробнее.
