---
layout: post
title: Выделения объектов для семантической сегментации при помощи "состязательного стирания". 
date: 2017-03-27
category: Computer vision
tags: [CV, CNN, semantic segmentation]
use_math: true
---

Задача семантической сегментации изображения заключается в том, чтобы назначить каждому пикселю этого изображения некоторый класс из заранее заданных.
Например, определить какие пиксели на изображении относятся к человеку, какие к сидящему у этого человека на коленях коту, а какие к некоему заднику
(background) и не могут быть классифицированы. Современный подход к решению этой задачи, как собственно и многих других, заключается в применении
свёрточных нейронных сетей. Этот подход показывает прекрасные результаты. Проблема, однако, как и всегда в случае применения нейронных сетей
заключается в необходимости получить значительные объемы, размеченных для тренировки, данных. При этом в случае семантической сегментации,
трудоёмкость задачи разметки крайне высока. Потому что для каждого изображения надо сформировать попиксельную маску классов. 

<!--more-->

На данный момент сформированно некоторое количество датасетов для тренировки сетей, решающих задачу семантической сегментации. Однако, их явно не
достаточно, в связи с чем постоянно появляются статьи, в которых авторы пытаются каким-то образом либо уменьшить трудоёмкость разметки, как, например,
в [1]. Где авторы используют игру "Grand Theft Auto V" как источник данных и за счёт внедрения специально написанных фильтров DirectX, получают знания
о текстурах и объектах, что позволяет "продлевать" разметку с одних кадров на другие. 

Второй вариант, использовать ослабленную разметку, например, уже существующие датасеты для тренировки классификационных сетей, где для
изображения нет попиксельной разметки, но известно какой основной объект изображен на картинке. Или для изображения задан набор
ограничиващих прямоугольников и соответствующих им классов (понятно, что такая разметка существенно менее трудоемка, чем попиксельная).

Еще один вариант, это перенести разметку с одного кадра в видео потоке на следующие. Здесь задача не только в том, чтобы получить методику такого
переноса, но и оценить насколько такие дополнительно размеченные кадры помогут улучшить качество работы сети. Такого рода исследование проведено,
например, в [2].

В [3] предложены две идеи как с использованием классификационной сети улучшить семантическую сегментацию.

### Состязательное стирание (adversarial erasing).

Суть подхода заключается в том, чтобы натренировать классификационную сеть, используя обычный набор картинок, для каждой из которых определен класс
изображенного на ней объекта. Данная сеть применяется, чтобы выделить наиболее дискриминативный регион, который определяет категорию объекта, для 
этого используется подход из [4]. В этот регион добавляются все пиксели вклад которых в результат классификации больше некоторого порога. Затем этот
регион "стирается" (заливается цветом средним по всем пикселям изображений набора). После чего сеть перетренировывается, и процесс повторяется. Всё 
это проделывается до тех пор, пока тренировка классификационной сети сходится. Поскольку мы постоянно ухудшаем тренировочные данные, через некоторое 
количество шагов натренировать сеть не удастся.

Авторы, в качестве примера, приводят случай, когда сеть классифицирует изображение с собакой. В начале, сеть для определения класса изображенного
объекта использует голову собаки. Затем голова затирается, и новая сеть использует тело собаки и т.д. 

В результате, мы имеем для изображения разметку объекта (которая объединяет стираемые пиксели). Однако, такая разметка может пропустить некоторые
регионы, принадлежащие объекту, и содержать шум на границах объекта. 

Чтобы учесть эти проблемы при тренировки сети предназначенной для сегментации, применяется следующий подход:

###  Prohibitive segmentation learning

Предлагается при тренировке сети для семантической сегментации использовать дополнительно классификационную сеть, которая будет выдавать веса для
каждого из классов, участвующих в сегментации. Соответственно, мы прогоняем изображение через классификационную сеть и получаем вектор, размерности 
равной количеству классов, элементы которого суть вероятности того, что объект на изображении принадлежит соотвествующему классу. Одновременно, тоже 
самое изображение проходит через сеть семантической сегментации и в результате для каждого пикселя мы получаем тоже вектор размерности по количеству 
классов, который уже определяет вероятность того, что пиксель принадлежит объекту класса. Теперь вектор полученный от классификационной сети мы 
умножаем по-элементно на вектор для каждого из пикселей, т.е. фактически используем в качестве весов вероятности, полученные в результате 
классификации. Для *background* класса используем единичный вес, т.е. не меняем вероятность данного класса, полученную от сегментационной сети.

Такое взвешивание позволяет увеличить вес классов, которые определились на данном изображении классификационной сетью, и уменьшить вес классов,
которые по мнению классификационной сети на данном изображении отсутствуют. Далее с учетом обновленных весов строятся маски классов на изображении и
считается ошибка как кросс-энтропия.

### Литература

1. *Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun. Playing for Data: Ground Truth from Computer Games. 2016*

2. *Siva Karthik Mustikovela, Michael Ying Yang, Carsten Rother. Can Ground Truth Label Propagation from Video help Semantic Segmentation? 
[arXiv:1610.00731](https://arxiv.org/abs/1610.00731)*

3. *Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan. Object Region Mining with Adversarial Erasing: A Simple 
Classification to Semantic Segmentation Approach. [arXiv:1703.08448](https://arxiv.org/abs/1703.08448)*

4. *B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. IEEE CVPR, 2016.*

5. *L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected
crfs. [arXiv:1412.7062](https://arxiv.org/abs/1), 2014.*
