---
layout: post
title: R-CNN, Fast R-CNN, Faster R-CNN etc.
date: 2017-06-14
category: Computer vision
tags: [CV, CNN, object detection, object classification, object localization]
use_math: true
---

Возвращаемся к задаче детектирования объектов на изображении. Один из подходов к решению данной задачи был рассмотрен, когда разбирали сеть 
[OverFeat]({% post_url 2017-02-19-overfeat_cnn %}). В данном тексте разберем конструкцию, которая впервые была предъявлена в статьях [1] и [2], а 
затем усовершенствована в статьях [3] и [4].

<!--more-->

## Содержание

- [R-CNN](#r-cnn-region-based-convolutional-network)

- [Fast R-CNN](#fast-r-cnn)

- [Faster R-CNN](#faster-r-cnn)

- [Литература](#литература)

---

## R-CNN (Region-based Convolutional Network)

Начнем со статей [1] и [2], там описана примерно одинаковая схема работы, однако, [2] более подробна и содержит в том числе описание *bounding box
regression*.

Авторы предлагают следующую схему работы:

1. На изображении выделяются, используя *selective search* (см. [5]), регионы, которые предположительно содержат объект.

2. Регион претендент (вернее BBox региона) при помощи аффиного преобразования отображается в квадрат *227х227*. При этом BBox перед 
   преобразованием расширяется таким образом, чтобы после преобразования получить вокруг региона претендента кайму шириной 16 пикселей.

3. Полученный прямоугольник подается на вход сети, описанной в статье [6]. Структура сети:

   ![Структура AlexNet]({{ site.baseurl }}/images/2017-03/alexnet.svg)

4. Со слоя *FC7* сети, снимается вектор особенностей размерности 4096, этот вектор используется в SVM, натренированной для каждого класса по схеме
   один против всех, для классификации.

5. Тот же вектор особенности подается в линейный регрессор соответствующий классу объекта для восстановления позиции объекта.

Заметим, что на шаге 3, можно выбрать другую структуру свёрточной сети, для генерации вектора особенностей, например, VGG16. Авторы приводят сравнение
использования двух различных сетей в статье [2] таблица 3. Мы в дальнейшем будем считать, для определенности, что используется AlexNet.

### Тренировка

Тренировка состоит из трёх частей.

1. Вначале сеть претренируется на ILSVRC 2012. При этом у нас в наборе данных нет точной разметки объектов для детектирования, т.е. мы тренируем
   только соответствие изображения и аннотации. Так же отметим, что тренируется сеть из [6] с классификационным слоем на 1000 классов, а не обрезанная
   до слоя *FC7*.

2. Fine-tuning. Заменяем последний полносвязный слой, чтобы на выходе получить вместо 1000 классов, 20 (позитивы) + 1 (негативы) из VOC. И 
   дотренировываем сеть на VOC датасете, полагая позитивами тех претендентов, у которых IoU с позитивом из ground truth больше чем 0.5 и негативами 
   все остальные. Минибатчи выбираются размера 128 элементов, состоящие из 32 позитивов и 96 негативов.

3. Тренируем для каждого класса SVM для классификации. Здесь за позитивы берутся BBox из ground truth, а за негативы те прямоугольники у которых IoU с
   позитивом из ground truth < 0.3 (подбирали порог на отрезке [0, 0.5] по сетке с шагом 0.1, при этом авторы отмечают, что порог 0.0 ухудшает mAP 
   на 4%, а порог 0.5 - на 5%). Отметим, что этот порог отличается от того, которые использовался при тренировки сети на шаге 2.

4. Для каждого класса строится линейный регрессор для уточнения позиции объекта.

### Тестирование

Вначале на тестовом изображении запускается *selective search*, который выбирает порядка 2000 претендентов. Затем каждый регион-претендент
отображают в квадрат размеров *227х227*, используя афинное преобразование и этот квадрат подаётся на вход сети, которая извлекает вектор особенностей.
Вектор особенностей последовательно подается в SVM натренированные для каждого класса и для каждого класса запускается обычный жадный NMS, который 
оставляет из двух претендентов у которых IoU (площадь пересечения деленная на площадь объединения) выше порога, только один с максимальной оценкой.

### SVM вместо softmax слоя нейронной сети.

Возникает вопрос зачем тренировать и использовать SVM, если уже есть натренированная нейронная сеть, последний слой (softmax) которой выдаёт 
вероятность того, что поданное изображение принадлежит одному из (20 + 1) классов. Авторы аргументируют эту замену тем, что качество, померянное на 
VOC 2007, в случае использования вместо SVM softmax слоя падает с 54.2% до 50.9% mAP. 

Объясняют это авторы комбинацией нескольких факторов. В том числе существенное влияние оказывает то, что при тренировке нейронной сети (на этапе 
fine-tuning-а) используется обычные негативы, тогда как в случае тренировки SVM используется подмножество *hard negatives*. Однако, в статье отмечают, 
что, применяя определенные трюки при fine-tuning, можно добиться от сети практически того же качества, которое получается при использовании SVM. 
Причем отказ от SVM даёт естественное увеличение скорости работы как на этапе тренировки так и на этапе непосредственно детектирования.

### Bounding-box regression

В статье [2] дополнительно рассказывается о тренировке линейной регрессии (своя модель для каждого из классов) для уточнения координат окна объекта, 
по вектору особенностей полученному с *MaxPool5* слоя нейронной сети (в сеть подаётся претендент полученный при помощи *selective search*). Такое 
уточнение локализации объекта позволяет повысить качество на 3-4%.

Рассмотрим два прямоугольника: 

1. $P = (P_x, P_y, P_w, P_h)$ - BBox претендента полученного из *selective search*. $P_x, P_y$ - координатами центра, $P_w, P_h$ - размеры.

2. $G = (G_x, G_y, G_w, G_h)$ - ground truth.

Обозначим $\phi_5(P)$ вектор особенностей получаемый на выходе слоя *MaxPool5* сети, когда в нее подаем изображение ограниченное прямоугольником $P$. 
Будем искать преобразование $P$ в $G$ в виде:

$ G_x = P_w d_x(P) + P_x$, 

$ G_y = P_h d_y(P) + P_y$,

$ G_w = P_w \exp(d_w(P))$,

$ G_h = P_h \exp(d_h(P))$,

при этом $ d_{\*}(P) = w_{\*}^T \cdot \phi_5(P)$ линейное преобразование вектора особенностей, а вектор $w_*$ ищем как решение задачи оптимизации:

$$ w_* = \underset{\hat{w}_*}{argmin} \sum_{i=0}^N \left (t_*^i - \hat{w}_*^T \phi(P^i) \right )^2 + \lambda \left \| \hat{w}_* \right \|^2$$

здесь мы берём набор пар $(P^i, G^i), i=1,...,N $ претендентов с соответствующим ground truth. А через $t_{\*}$ обозначаем соответственно:

$ t_x = (G_x - P_x) / P_w $

$ t_y = (G_y - P_y) / P_h $

$ t_w = \ln (G_w / P_w) $

$ t_h = \ln (G_h / P_h) $


### Итоги

Подводя итог. Схема работы, описанная в статьях [1] и [2] выглядит следующим образом:

![Полная схема R-CNN ]({{ site.baseurl }}/images/2017-06/rcnn.svg)

## Fast R-CNN

Статья [3] является продолжением предыдущих двух. R-CNN описанная ранее имеет несколько недостатков, в основном связанных с высокими временными
затратами. 

1. Требуется натренировать свёрточную нейронную сеть (в два этапа: первый - тренировка без точной разметки и второй - fine-tuning для 20 классов), 

2. Требуется натренировать набор SVM по одной для каждого класса. При этом на вход SVM принимает вектора особенностей полученные от сети из п.1,
   которые надо для начала вычислить на наборе данных используемых для тренировки.

3. Требуется натренировать линейные регрессоры (опять же по одному для каждого класса), для восстановления позиций объекта.

4. Непосредственно процесс детектирования и классификации требует существенного времени (авторы пишут о 47 секундах на одно изображение при
   использовании VGG16 в качестве свёрточной сети)

Если с медленным и сложным процессом тренировки можно смириться, то существенные временные затраты на этапе детектирования сильно снижают практическую
пользу данного подхода. Причина из-за которой детектирование работает так медленно, заключается в том, что при помощи selective search генерируется
примерно 2000 претендентов на изображении, и каждый претендент должен быть пропущен через свёрточную сеть, чтобы вычислить для него вектор 
особенностей. Процесс этот достаточно затратный, особенно для сетей с большим числом свёрточных слоёв. Поэтому, авторы, 
вдохновленные подходом, применяемом в SPPNet (c SPPNet мы уже разбирались, см. 
[Spatial Pyramid Pooling структура в свёрточной нейронной сети]({% post_url 2017-03-08-spp_net %})), предлагают подавать на вход сети полное
изображение, но при этом последний max-pool слой заменить на слой нового типа *RoI pooling*.

*RoI pooling* слой принимает на вход карту особенностей, полученную от последнего свёрточного слоя нейронной сети, и RoI претендента
(в координатах изображения). RoI преобразуется из координат изображения в координаты на карте особенностей и на полученный прямоугольник накладывается 
сетка *W x H* с наперед задаными размерами (например, для VGG16 авторы статьи предлагают *W = H = 7*). Делается *max pooling* по каждой ячейке этой 
сетки. Таким образом *RoI pooling* слой преобразует, аналогично тому как это делалось в SPP, вектор особенностей произвольного прямоугольника из
исходного изображения в вектор особенностей фиксированной размерности.

После *RoI pooling* слоя данные через два полносвязных слоя подаются параллельно на:

а. *softmax* слой для оценки принадлежности данного претендента одному из классов объектов, 

б. слой реализующий регрессию, которая уточняет BBox объекта.

Таким образом получаем следующую схему. Берется свёрточная нейронная сеть натренированная на ImageNet датасете (AlexNet, VGG и т.п.), последний пулинг
слой заменяется RoI пулинг, добавляется два обычных полносвязных слоя и от них параллельно еще пара слоёв:

![Последнии слои Fast R-CNN ]({{ site.baseurl }}/images/2017-06/fast_rcnn_lastlayers.svg)

### Тренировка

#### Функция потерь

Чтобы тренировать полученную сеть, необходимо ввести функцию потерь, объединяющую ошибки классификации и регрессии координат прямоугольника. Обозначим
через $p = (p_0, p_1, ..., p_N) $ - выход с *softmax* слоя - вектор вероятностей, что в соответствующем RoI содержится объект одного из $N + 1$
классов. Второй слой выдаст нам матрицу $4 \times N$ восстановленных сдвигов bbox-а объекта для каждого из $N$ классов: $(t^n_x, t^n_y, t^n_w, t^n_h)$,
$n=1,...,N$. Здесь координаты bbox-а восстанавливаются по сдвигам аналогично тому как это было в случае обычной R-CNN, только там мы использовали
набор линейных регрессий для поиска $t$. 

Авторы предлагают следующую функцию потерь:

$$ L(p, u, t^u, v) = L_{cls}(p, u) + \lambda [u \geq 1] L_{loc}(t^u, v)$$

здесь $u$ - класс переданного RoI по версии датасета, $v = (v_x, v_y, v_w, v_h)$ - bbox объекта, соответствующего переданному RoI. $u=0$ соответствует
классу негативов, и в этом случае второе слагаемое в функции потерь зануляется. $\lambda$ позволяет балансировать влияние ошибки классификации и 
ошибки уточнения координат. 

Ошибка классификации вычисляется стандартно: $L_{cls}(p, u) = -\log(p_u)$. Во втором слагаемом в качестве функции потерь берется следующая сумма:

$$L_{loc}(t^u, v) = \sum_{i = x, y, w, h}\Psi_1(t^u_i - v_i)$$

через $\Psi_1(x)$ обозначена функция Хубера (с параметром 1):

$$
\Psi_1(x) = 
\begin{cases}
  0.5 x^2, & {\rm если}|x| < 1 \\
  |x| - 0.5,   & {\rm иначе}
\end{cases}
$$

Функция Хубера менее чувствительна к выбросам, чем просто $L_2$ норма и это позволяет избежать тонкой настройки learning rate.

#### Минибатч

Чтобы уменьшить время тренировки, в минибатч лучше класть несколько претендентов из одного изображения, это позволяет переиспользовать карту
особенностей, а также промежуточные данные для обратного распространения. Однако, не стоит злоупотреблять и составлять минибатч только из претендентов с
одного изображения, потому что их коррелированность может замедлить сходимость градиентного спуска. В статье предлагается следующая схема:

1. Выбирается случайным образом *N* изображений

2. На каждом выбирается *R / N* претендентов.

Таким образом получаем минибатч составленный из *R* элементов. Авторы статьи пишут, что если использовать *N = 2* и *R = 128* скорость тренировки по 
грубой оценке в 64 раза выше, чем если набирать минибатч выбирая по одному претенденту из 128 различных изображений.

Итак авторы предлагают использовать параметры *N = 2* и *R = 128*. Соотношение количества позитивов и негативов предлагают оставить таким же которое
использовалось при тренировки R-CNN. Т.е. набирают 25% позитивов и 75% негативов. В качестве позитивов берутся те претенденты у которых метрика IoU
(пересечение деленное на объединение) с некоторым объектом на данном изображении больше чем 0.5. Негативами считаются соответственно претенденты у
которых эта метрика лежит в пределах от 0.1 до 0.5. Претенденты у которых IoU с объектами меньше 0.1 не используются, из предположения, что таким
образом в тренировку включаются только, в некотором смысле, *hard negatives*.

### Truncated SVD

На самом деле статью [3] есть смысл прочитать целиком, там рассказывает о некоторых интересных трюках в плане как тренировать и как расширять или не
расширять данные для тренировки. Но мы здесь поговорим об одном методе, который позволяет ускорить работу на этапе детектирования.

Когда свёрточная сеть используется для классификации изображения, то основные вычисления и следовательно основные временные затраты лежат на
свёрточных слоях. Однако, в случае, когда мы на одном изображении детектируем и классифицируем несколько объектов и для этого вынуждены рассмотреть 
тысячи претендентов, основные затраты времени переходят на полносвязные слои. Для ускорения работы используется следующий подход. Допустим у нас есть
полносвязный слой с матрицей весов $W$ размера $u \times v$. Зададимся некоторым $t$ меньшим $u$ и $v$ и, используя сингулярное разложение матрицы $W$,
приблизим ее произведением:

$$ W \approx U \Sigma_t V^T $$

здесь $\Sigma_t$ - диагональная матрица размеров $t \times t$, на диагонали которой $t$ максимальных сингулярных чисел матрицы $W$, $U$ - матрица 
размера $u \times t$ соответствующих левых сингулярных векторов, а $V$ - размера $v \times t$ соответствующих правых сингулярных векторов.

Теперь заменяем полносвязный слой с матрицей $W$ на два полносвязных слоя (без нелинейности между ними), первый с матрицей весов $\Sigma_t V^T$,
второй с матрицей весов $V$. Количество операций при этом уменьшается с $u\cdot v$ до $(u + v) \cdot t$. Это даёт ускорение при большом количестве
претендентов на одно изображение, и при этом не сильно уходшает качество (авторы пишут о 30% увеличении скорости при потери 0.3% mAP).

### Итоги

Итак в Fast R-CNN авторы предложили шаги для существенного ускорения процессов тренировки и использования детектора. Схема работы превратилась в
следующую:

![Полная схема Fast R-CNN ]({{ site.baseurl }}/images/2017-06/fast_rcnn.svg)

Во-первых, в отличии от R-CNN, где карта особенностей генерировалась для каждого претендента на изображении, в данном случае генерируется карта 
особенностей для всего изображения, а затем при помощи специального слоя из нее вычленяются карты для каждого из претендентов, что позволяет 
существенно сократить время вывода. Во-вторых, авторы отказались от тренировки отдельного SVM для каждого класса и научились тренировать сеть таким 
образом, чтобы использование softmax слоя давало результаты сопоставимые с использованием набора SVM. В-третьих, авторы отказались от использования 
отдельных линейных регрессоров и натренировали, для уточнения локализации объектов, отдельный слой нейронной сети.

## Faster R-CNN

Следующая статья [4] предлагает заменить процедуру генерации претендентов *selective search* на небольшую нейронную сеть, которая использует имеющуюся
карту особенностей. В качестве свёрточной сети авторы предлагают использовать либо [VGG16]({{ site.base.url }}/images/2017-06/vgg16.svg) (см. [7]),
либо ZF (см. [8]), мы для определенности будем рассматривать случай VGG16 (для ZF ситуация будет аналогична). Для изображения размера $W_I \times H_I$
на выходе последнего свёрточного слоя (*conv5_3*) сеть VGG16 выдает карту особенностей с пространственными размерами $W_I / 16 \times H_I / 16$,
вектор особенностей для каждой точки будет размерности 512. При этом в вектор особенностей в точке $(x_f, y_f)$ вносят вклад точки изображения
лежащие внутри квадрата с центром в $(16 x_f, 16 y_f)$ и размера $196 \times 196$

> #### Замечание.
> 
> Действительно, возьмём точку с координатами $(x_f, y_f)$ на выходе слоя *conv5_3*, тогда значение вектора в этой точке обусловлено квадратом с
> центром в точке с теми же координатами и размерами $3 \times 3$ на входе этого слоя (или, что тоже самое на выходе слоя *conv5_2*), продолжая 
> движение назад по сети на выходе слоя *conv5_1* будет квадрат $(x_f, y_f), 5 \times 5$ и т.д. 
>
> Получаем следующую цепочку:
>
> | слой     |  выход (координаты центра и размеры |
> |----------|:------------------------------------|
> | conv5_3  | $(   x_f,    y_f)$, $1 \times 1$      |
> | conv5_2  | $(   x_f,    y_f)$, $3 \times 3$      |
> | conv5_1  | $(   x_f,    y_f)$, $5 \times 5$      |
> | pool4    | $(   x_f,    y_f)$, $7 \times 7$      |
> | conv4_3  | $( 2 x_f,  2 y_f)$, $14 \times 14$    |
> | conv4_2  | $( 2 x_f,  2 y_f)$, $16 \times 16$    |
> | conv4_1  | $( 2 x_f,  2 y_f)$, $18 \times 18$    |
> | pool3    | $( 2 x_f,  2 y_f)$, $20 \times 20$    |
> | conv3_3  | $( 4 x_f,  4 y_f)$, $40 \times 40$    |
> | conv3_2  | $( 4 x_f,  4 y_f)$, $42 \times 42$    |
> | conv3_1  | $( 4 x_f,  4 y_f)$, $44 \times 44$    |
> | pool2    | $( 4 x_f,  4 y_f)$, $46 \times 46$    |
> | conv2_2  | $( 8 x_f,  8 y_f)$, $92 \times 92$    |
> | conv2_1  | $( 8 x_f,  8 y_f)$, $94 \times 94$    |
> | pool1    | $( 8 x_f,  8 y_f)$, $96 \times 96$    |
> | conv1_2  | $(16 x_f, 16 y_f)$, $192 \times 192$  |
> | conv1_1  | $(16 x_f, 16 y_f)$, $194 \times 194$  |
> | image    | $(16 x_f, 16 y_f)$, $196 \times 196$  |
>

Для каждой точки карты особенностей $(x_f, y_f)$ будем проверять *k* претендетов разных размеров на изображении в регионах с центром в 
$(16 x_f, 16 y_f)$. В статье авторы предлагают рассматривать 9 претендентов, варьируя три масштаба и три отношения сторон ($1 : 1$, $1 : 2$, $2 : 1$).
Для задачи будем использовать *Region Proposal Network (RPN)*.

![Схема Region Proposal Network]({{ site.baseurl }}/images/2017-06/faster_rcnn_rpn.svg)

Как видно из схемы, карта особенностей, полученная от свёрточной сети подается на свёрточный слой с ядром размера $3 \times 3$. А выход этого
свёрточного слоя параллельно подается на два свёрточных слоя с ядром размера $1 \times 1$. Первый слой *rpn_cls_score* выдает *k* пар - вероятности
наличия или отсутствия объекта в соответствующем региона (фактически мы имеем обычный полносвязный классификатор, который применяется для каждого
вектора особенностей сформированного слоем *rpn_conv*). Слой *rpn_bbox_pred* выдает *k* четверок - поправки для координат центра и размеров
соответствующего региона претендента (это получется полносвязный регрессор, который опять применяется к векторам особенностей). Отметим, что данный
генератор претендентов, в силу применяемой схемы, инвариантен к сдвигам объектов на изображении.

Авторы называют четвёрку: две координаты центра, масштаб и отношение сторон - анкер (*anchor*). Анкер полностью определяет регион на
изображении. Количество всевозможных анкеров для изображения размеров $W_I \times H_I$ будет равно 
$9 \cdot \left\lfloor \frac {W_I}{16} \right\rfloor \cdot \left\lfloor \frac {H_I}{16} \right\rfloor$

### Тренировка

Фактически необходимо натренировать две сети: *Fast R-CNN* для классификации и уточнения координат объектов, RPN для генерации претендентов. При этом
свёрточные слои должны быть общими для этих двух сетей (собственно, всё и затевалось ради того, чтобы суметь использовать одну карту особенностей для
решения обеих задач). 

Авторы предлагают несколько возможных подходов к тренировке, однако, результаты, представленные в статье, получены при помощи тренировки по следующей
схеме:

1. Тренируем сеть для генерации претендентов. Для этого инициализируем свёрточные слои весами натренированными на ImageNet и доуточняем веса 
   свёрточных слоёв и слоёв RPN части (подробно про процесс тренировки этой сети чуть ниже).

2. Тренируем сеть *Fast R-CNN* сеть. Свёрточные слои инициализируем весами от сети, натренированной на ImageNet и дотренировываем (процесс тренировки 
   *Fast R-CNN* мы уже разбирали в соотвествующей части данного текста). При этом претенденты для изображения генерируем при помощи сети полученной в 
   п.1. 

   Важно, что после выполнения п.2 мы имеем две нейронных сети с разными весами для свёрточных слоёв.

3. Берем свёрточные слои от *Fast R-CNN*, которую натренировали в п.2, переносим их в *RPN* сеть и дотренировываем её. При этом веса свёрточных слоёв
   фиксируются, а тренируются только веса слоёв *rpn_conv*, *rpn_cls_score* и *rpn_bbox_pred*.

4. Дотренировываем *Fast R-CNN* с учетом изменившегося генератора претендентов. При этом опять не меняем веса свёрточных слоёв, и тренируем только слои
   специфичные для *Fast R-CNN*.

На этом тренировка заканчивается. Осталось разобраться с тем как тренировать сеть генерирующую претендентов.

### Тренировка RPN

Тренировка осуществляется обычным SGD. Минибатчи собираются из претендентов (анкеров) случайным образом выбранных на одном изображении (для ускорения 
тренировки), размер минибатча выбирают равным 256, количество позитивных и негативных претендентов в минибатче пытаются сделать одинаковым, если при
этом не удаётся набрать достаточное количество позитивных претендентов, то до 256 экземпляров добираются негативные. 

Анкер считается позитивом если метрика IoU этого анкера с некоторым объектом из ground truth данного изображения больше 0.7 или если этот анкер имеет
максимальную метрику IoU среди всех анкеров для данного объекта из ground truth. Авторы пишут о том, что крайне редко, но случается, что ни один анкер
не имеет с каким-то объектом IoU выше 0.7, чтобы разрешить эту ситуацию добавлен второй вариант с максимальной метрикой. Заметим, что один объект на
изображении может выдать несколько позитивных анкеров.

Негативами считаются анкеры у которых метрика IoU для всех объектов на изображении меньше 0.3. Те анкеры, которые не попали ни в позитивы ни в
негативы игнорируются и в тренировке участия не принимают. 

Функция потерь для тренировки *RPN*, как и функция потерь для тренировки *Fast R-CNN* состоит из двух частей:

$$ L(\{p_i\}, \{t_i\}) = \sum_i L_{cls}(p_i, p_i^*) + \lambda \sum_i p_i^* L_{reg}(t_i, t_i^*)$$

здесь $p_i$ - вероятность того, что $i$-ому анкеру соответствует объект, полученная с выхода слоя *rpn_cls_score*. $p_i^{\*}$ - равна единице, если 
$i$-ый анкер соотвествует позитиву в минибатче, и соотвественно $L_{cls}(p_i, p_i^{\*})$ логистическая функция потерь на два класса (объект или не 
объект). Во втором слагаемом сумма считается только по тем анкерам, которые соотвествуют объектам ($p_i^{\*} = 1$). $L_{reg}$ это таже сумма функций
Хубера, что использовалась при тренировки *Fast R-CNN*. 

Более формально. Обозначаем $(x_a, y_a)$ - координаты центра анкера, а $w_a, h_a$ его размеры, 
$(x, y)$ - координаты центра и $(w, h)$ - размеры прямоугольника предсказанного сетью, и наконец, $(x^{\*}, y^{\*})$, $w^{\*}, h^{\*}$ - координаты
центра и размеры прямоугольника ground truth объекта на изображении, соответствующего анкеру. Вычисляем:

$ t_x = (x - x_a) / w_a $, $ t^{\*}_x = (x^{\*} - x_a) / w_a $,

$ t_y = (y - y_a) / h_a $, $ t^{\*}_y = (y^{\*} - y_a) / h_a $,

$ t_w = \ln (w / w_a) $, $ t^{\*}_w = \ln (w^{\*} / w_a) $,

$ t_h = \ln (h / h_a) $, $ t^{\*}_h = \ln (h^{\*} / h_a) $

И получаем:

$$L_{reg}(t, t^*) = \sum_{i = x, y, w, h} \Psi_1(t_i - t^*_i)$$

Параметр $\lambda$ позволяет балансировать вклад в функцию штрафов ошибки определения содержит ли регион объект и ошибки уточнения координат объекта.

### Итоги

Итак в статье [4] описана нейронная сеть для детектирования и классификации объектов. Теперь весь процесс реализован одной нейронной сетью.

![Полная схема Faster R-CNN ]({{ site.baseurl }}/images/2017-06/faster_rcnn.svg)

---
 
### Литература

1. *R. Girshick, J. Donahue, T. Darrell, and J. Malik. "Rich feature hierarchies for accurate object detection and semantic segmentation."
In CVPR, 2014. [arXiv:1311.2524](https://arxiv.org/abs/1311.2524)*

2. *R. Girshick, J. Donahue, T. Darrell, and J. Malik. "Region-based convolutional networks for accurate object detection and segmentation."
TPAMI, 2015*

3. *R. Girshick, "Fast R-CNN," in IEEE International Conference on Computer Vision (ICCV), 2015.*

4. *S. Ren, K. He, R. Girshick, and J. Sun, "Faster R-CNN: Towards real-time object detection with region proposal networks," in Neural Information 
Processing Systems (NIPS), 2015.*

5. *J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. "Selective search for object recognition." IJCV, 2013.*

6. *A. Krizhevsky, I. Sutskever, and G. Hinton. "ImageNet classiﬁcation with deep convolutional neural networks." In NIPS, 2012.*

7. *K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.*

8. *M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional neural networks,” in European Conference on Computer Vision (ECCV),
2014.*
