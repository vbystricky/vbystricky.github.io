---
layout: post
title: Нормализация в нейронных сетях.
date: 2024-01-02
category: Neural network
tags: [neural network, normalization]
use_math: true
published: true
send2tg: true
---

В своё время мы разбирались с [batch normalization]({% post_url 2020-05-31-batch_normalization %}), заметив, что ее использование приносит большую
пользу при тренировке. В этот раз попробуем собрать вместе несколько (все, которые мне известны на данный момент) вариантов *нормализации признаков*.

<!--more-->

## Регуляризация данных и ускорение тренировки

Первый набор нормализаций в основном предназначен для регуляризации данных, перемещающихся от слоя к слою нейронных сетей. Вводя такие дополнительные
преобразования мы пытаемся ускорить тренировку и получить возможность не особо задумываться о том, как инициализировать начальные веса сети. Все
методы, рассматриваемые в данном разделе, кроме LRN, реализуют вариации одного и того же алгоритма, который изначально был предложен в [2]. Получив
тензор после какого-то из слоёв мы:

1. считаем статистики этого тензора (мат. ожидание и дисперсию)

2. сдвигаем среднее в ноль и дисперсию в единицу (опрощенное [whitening]({% post_url 2020-05-31-batch_normalization %}#batch-normalization-определение)
преобразование). 

3. производим афинное преобразование признаков параметры которого тренируются вместе с весами сети.

В разных вариантах нормализации меняются только оси вдоль которых считаются статистики в п.1. Наглядно это можно изобразить следующим образом:

![варианты нормализации наглядно]({{ site.baseurl }}/images/2024-01/normalization.svg)

Начнем мы, однако, со способа, который не вписывается в данную схему.

### Local Response Normalization (LRN)

Первый вариант нормализации, который мы рассмотрим, был предложен в 2012 году в статье [1]. В то время нейронные сети рассматривали в тесной связи с
биологическими нейронами и этот метод был "подсмотрен" в работе мозга, и имитировал латеральное торможение (*lateral inhibition*) - способность
возбужденного нейрона снижать активность своих соседей.

Отметим, что в исходной статье речь идет о свёрточной нейронной сети и работе с изображениями, "близкими" нейронами, которые группируются и
тормозят активность друг друга, считаются нейроны находящиеся в одном изображении, в одной и той же пространственной точке, и в близких каналах.

Если после очередного сверточного слоя имеем тензор размерности $(N, H^{(i)}, W^{(i)}, C^{(i)})$ (здесь $N$ - размер батча, $H^{(i)}, W^{(i)}$ - высота
и ширина нормализуемого слоя, $C^{(i)}$ - число каналов/признаков на данном слое), формально локальная нормализация записывается как:

$$
\hat{a}_{n, y, x, c} = a_{n, y, x, c}/\left(k + \alpha\sum_{j=\max(0, c - r/2)}^{\min(C-1, c + r/2)} a_{n, y, x, j}^2\right)^{\beta}
$$

$r$ здесь определяет окрестность по оси признаков, в которой мы набираем нейроны влияющие на данный. В исходной статье [1] гиперпараметры выбираются 
$k=2$, $r=5$, $\alpha=10^{-4}$, $\beta=0.75$. 

Некоторая "странность" данного варианта нормализации заключается в произвольности порядка признаков, т.е. не вполне понятно, почему близкие признаки
должны влиять друг на друга. С другой стороны, это можно объяснить тем, что в процессе тренировки, мы действительно за счет такой дополнительной
нормализации будем "собирать" взаимновлияющие признаки рядом. 

В каком-то смысле более логичный вариант LRN (который называют *intra-channel LRN*, в отличии от *inter-channel LRN* рассмотренного выше), можно
получить, если окрестность определять по пространственным осям:

$$
\hat{a}_{n, y, x, c} = a_{n, y, x, c}/\left(k + \alpha\sum_{i=\max(0, x - r/2)}^{\min(W-1, x + r/2)}\,\sum_{j=\max(0, y - r/2)}^{\min(H-1, y + r/2)} a_{n, j, i, c}^2\right)^{\beta}
$$


*Замечание.
На самом деле, нет никакой особой причины, не использовать inter-channel LRN для обычных полносвязных сетей, работать всё будет ровно так же только
исчезнут геометрические размерности $x$ и $y$.*


### Batch normalization (BN)

C [batch normalization]({% post_url 2020-05-31-batch_normalization %}) (см. [2]) мы разбирались достаточно подробно, поэтому здесь только коротко
повторим основные моменты. Для выхода с линейного слоя, до функции активации, считаем математическое ожидание и дисперсию для каждого признака и 
нормализуем признаки:

$$
\hat a_{n, c} = \frac {(a_{n,c} - E_n(a_{n,c}))} {\sqrt{Var_n(a_{n, c})} + \varepsilon} 
$$

На самом деле мы во время тренировки имеем только минибатч (размера $B$), поэтому мат. ожидание и дисперсию считаем по этому минибатчу:

$$
\mu_{c} = \frac 1 B \sum_{n=0}^{B-1} a_{n, c} 
$$

$$
\sigma_{c} = \sqrt{\frac 1 B \sum_{n=0}^{B-1} (a_{n, c} - \mu_{c})^2} 
$$

и вместе с весами сети сохраняем, экспоненциальные среднии этих $\mu_{c}$ и $\sigma_{c}$ за все время тренировки (отдельные для каждого слоя и для
каждого признака выходящего из этого слоя).

Окончательное преобразование (т.е. тензор, который уйдет в функцию активации) выглядит так:

$$
b_{n,c} = \gamma_{c} \cdot \hat a_{n, c} + \beta_{c} = \frac {\gamma_{c}} {\sigma_{c} + \varepsilon} (a_{n,c} - \mu_{c})  + \beta_c
$$

Веса $\gamma_{c}$ и $\beta_{c}$ подбираются во время тренировки. Также возникают два гиперпараметра: $\varepsilon$ - которая защищает от деления
на нуль, при нулевой дисперсии признака, и коэффициент экспоненциального среднего для получения мат.ожидания и дисперсии используемых при выводе сети.

BN хорошо обобщается на случай работы с изображениями - просто при подсчете моментов усредняем по всем осям, кроме оси признаков:

$$
\mu_{c} = \frac 1 {N \cdot W \cdot H} \sum_{n=0}^{N-1}\sum_{y=0}^{H-1}\sum_{x=0}^{W-1} a_{n, y, x, c}
$$

$$
\sigma_{c} = \sqrt{\frac 1 {N \cdot W \cdot H} \sum_{n=0}^{N-1}\sum_{y=0}^{H-1}\sum_{x=0}^{W-1} (a_{n, y, x, c} - \mu_{c})^2} 
$$

$$
b_{n,y,x,c} = \gamma_{c} \cdot \hat a_{n,y,x,c} + \beta_{c} = \frac {\gamma_{c}} {\sigma_{c} + \varepsilon} (a_{n,y,x,c} - \mu_{c})  + \beta_c
$$

### Layer normalization (LN)

BN - очень полезная методика, позволяющая существенно сократить время тренировки, увеличить learning rate и относительно вольно инициализировать веса
сети, однако, имеются и некоторое недостатки: во-первых, желательно, чтобы минибатчи были достаточно большого размера, а, во-вторых, есть определенные
трудности с использованием BN в [рекурентных нейронных сетях]({% post_url 2021-05-10-rnn_lstm_gru_etc %}) (на самом деле применять BN в рекурентных
сетях можно, см., например, [3, 4], только делать это надо с известной долей аккуратности, т.е. если просто добавить обычный BN слой, то результаты
скорее ухудшаться). Чтобы побороться с этими проблемами, в [5] был предложен новый вариант нормализации - layer normalization.

Собственно, LN работает практически так же как BN, только мат. ожидание и дисперсия считаются не вдоль оси батча, а вдоль оси признаков. Т.е. мы
оставляем преобразование:

$$
b_{n,c} = \frac {\gamma_{c}} {\sigma_{n} + \varepsilon} (a_{n,c} - \mu_{n})  + \beta_c
$$

Но среднее и дисперсию считаем для каждого тренировочного сэмпла отдельно:

$$
\mu_{n} = \frac 1 {C} \sum_{c=0}^{C-1} a_{n, c}
$$

$$
\sigma_{n} = \sqrt{\frac 1 {C} \sum_{c=0}^{C-1} (a_{n, c} - \mu_{n})^2} 
$$

Таким образом, мы избавляемся от проблемы с размером мини-батча - LN от него не зависит и работает даже для мини-батчей размера 1, так же LN 
отлично работает в рекурентных сетях, и, наконец, нет необходимости подсчитывать скользящее среднее для мат. ожидания и дисперсии.

LN можно использовать и для работы с изображениями, в этом случае статистики считаем целиком по тензору, соответствующему одному тренировочному
сэмплу, т.е. по всем трем осям:

$$
\mu_{n} = \frac 1 {W \cdot H \cdot C} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1}\sum_{c=0}^{C-1} a_{n, y, x, c}
$$

$$
\sigma_{n} = \sqrt{\frac 1 {W \cdot H \cdot C} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1}\sum_{c=0}^{C-1} (a_{n, y, x, c} - \mu_{n})^2} 
$$

а следующее преобразование снова аналогично BN:

$$
b_{n,y,x,c} = \frac {\gamma_{c}} {\sigma_{n} + \varepsilon} (a_{n,y,x,c} - \mu_{n})  + \beta_{c}
$$

*Замечание. LN от BN отличается только подсчетом статистик. Параметры $\gamma_{c}$ и $\beta_{c}$ имеют те же размеры (по числу признаков) и тот же
смысл как для BN так и для LN.*

### Group Normalization (GN)

В [7] авторы решили совместить LN и идею LRN. Т.е. разбить признаки на группы и считать мат. ожидание и дисперсию по признакам из каждой группы. Если
мы возьмём $G$ групп (желательно, чтобы $C$ было кратно $G$), то в каждой группе у нас будет $M = C / G$ признаков, и формулы для моментов будут
выглядеть так:  

$$
\mu_{n, g} = \frac 1 {M} \sum_{c=g\cdot M}^{(g + 1) \cdot M - 1} a_{n, c}
$$

$$
\sigma_{n, g} = \sqrt{\frac 1 {M} \sum_{c=g\cdot M}^{(g + 1) \cdot M - 1} (a_{n, c} - \mu_{n,g})^2} 
$$

а преобразование GN:

$$
b_{n, c} = \frac {\gamma_{c}} {\sigma_{n, g} + \varepsilon} (a_{n,c} - \mu_{n, g})  + \beta_{c},\, g = \left\lfloor \frac c M \right\rfloor
$$

здесь мы для признака с индексом $c$ используем мат. ожидание и дисперсию посчитанные по той группе признаков в которую он попадает: 
$g = \left\lfloor \frac c M \right\rfloor$

Для GN верно тоже замечание, которое было сделано для LN: отличие от BN только в подсчете статистик, параметры $\gamma_{c}$ и $\beta_{c}$ снова имеют
те же размеры и тот же смысл, что и для BN или LN.

GN легко обобщается на случай работы с изображениями. Мат. ожидание и дисперсию считаем по формулам:

$$
\mu_{n, g} = \frac 1 {M \cdot W \cdot H} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1} \sum_{c=g\cdot M}^{(g + 1) \cdot M - 1} a_{n, y, x, c}
$$

$$
\sigma_{n, g} = \sqrt{\frac 1 {M \cdot W \cdot H} \sum_{y=0}^{H-1} \sum_{x=0}^{W-1} \sum_{c=g\cdot M}^{(g + 1) \cdot M - 1} (a_{n, y, x, c} - \mu_{n,g})^2} 
$$

а само преобразование:

$$
b_{n, y, x, c} = \frac {\gamma_{c}} {\sigma_{n, g} + \varepsilon} (a_{n, y, x, c} - \mu_{n, g})  + \beta_{c},\, g = \left\lfloor \frac c M \right\rfloor
$$

Отметим, что если в GN положить число групп $G = 1$, то мы получим просто layer normalization.

Для полносвязных слоёв BN, LN и GN исчерпывают подход с нормализацией, которая работает по схеме, описанной в начале раздела.

Однако, когда размерностей у тензора больше двух, т.е. когда кроме размерности признаков и размерности количества данных (размерности мини-батча) у
нас есть и другие измерения, например, если мы работаем с изображениями и кроме батча и каналов имеем две пространственные оси, мы можем считать
статистики вдоль этих осей и, соответственно, получать другие нормализации. В качестве еще одной такой вариации рассмотрим instance normalization,
которая получается из GN, если взять число групп $G$ равным числу признаков $C$. Очевидно, что использовать такой вариант для полносвязных слоёв
бессмысленно, поскольку мы будем вынуждены считать статистику просто по одному числу - значению признака, а вот для изображений мы в этом случае
считаем мат. ожидание и дисперсию по каждому каналу и это работает.

### Instance normalization (IN)

Instance normalization вводится в статье [7], которая решает задачу переноса стиля изображения. Предлагается мат. ожидание и дисперсию считать только
по пространственным осям:

$$
\mu_{n, c} = \frac 1 {W \cdot H} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1} a_{n, y, x, c}
$$

$$
\sigma_{n, c} = \sqrt{\frac 1 {W \cdot H} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1} (a_{n, y, x, c} - \mu_{n})^2} 
$$

Т.е. для каждого канала каждого из изображений в батче считаются свои отдельные статистики. Сама формула нормализации при этом не отличается от BN или
LN:

$$
b_{n,y,x,c} = \frac {\gamma_{c}} {\sigma_{n, c} + \varepsilon} (a_{n,y,x,c} - \mu_{n, c})  + \beta_{c}
$$

$\gamma_{c}$ и $\beta_{c}$ как обычно имеют размер по числу признаков (=каналов) соответствующего слоя.

IN заменяют BN в декодерах генеративных свёрточных нейронных сетей при работе с изображениями. Исходная статья [7] предлагает делать это для декодера
в задаче переноса стиля одного изображения на другое.

### Switchable normalization (SN)

Следующий вариант нормализации (см. [8]) является взвешенным объединением IN, LN, и BN.

Подсчитаем все три варианта статистик,  оптимально это можно сделать, вначале подсчитав статистики IN, а затем на их базе досчитав LN и BN:

$$
\mu^{(in)}_{n, c} = \frac 1 {W \cdot H} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1} a_{n, y, x, c}, \;
\sigma^{(in)}_{n, c} = \sqrt{\frac 1 {W \cdot H} \sum_{y=0}^{H-1}\sum_{x=0}^{W-1} (a_{n, y, x, c} - \mu^{(in)}_{n})^2} 
$$

$$
\mu^{(ln)}_{n} = \frac 1 {C} \sum_{c=0}^{C-1}\mu^{(in)}_{n, c}, \;
\sigma^{(ln)}_{n} = \sqrt{\frac 1 {C} \sum_{c=0}^{C-1}\left(\left(\sigma^{(in)}_{n, c}\right)^2 + \left(\mu^{(in)}_{n, c}\right)^2\right) - \left(\mu^{(ln)}_{n}\right)^2} 
$$

$$
\mu^{(bn)}_{c} = \frac 1 {N} \sum_{n=0}^{N-1} \mu^{(in)}_{n, c}, \;
\sigma^{(bn)}_{c} = \sqrt{\frac 1 {N} \sum_{n=0}^{N-1}\left(\left(\sigma^{(in)}_{n, c}\right)^2 + \left(\mu^{(in)}_{n, c}\right)^2\right) - \left(\mu^{(bn)}_{c}\right)^2} 
$$

будем использовать все три пары мат. ожиданий и дисперсий, но с разными весами:

$$
\mu_{n, c} = \omega^{(in)} \mu^{(in)}_{n, c} + \omega^{(ln)} \mu^{(ln)}_{n} + \omega^{(bn)} \mu^{(bn)}_{c},
$$

$$
\sigma_{n, c} = \sqrt{\psi^{(in)} (\sigma^{(in)}_{n, c})^2 + \psi^{(ln)} (\sigma^{(ln)}_{n})^2 + \psi^{(bn)} (\sigma^{(bn)}_{c})^2}
$$

результирующие преобразование будет иметь наш обычный вид:

$$
b_{n,y,x,c} = \frac {\gamma_{c}} {\sigma_{n, c} + \varepsilon} (a_{n,y,x,c} - \mu_{n, c})  + \beta_{c}
$$

при этом мы хотим иметь выпуклую линейную комбинацию статистик, т.е.

$$
\omega^{(in)} + \omega^{(ln)} + \omega^{(bn)} = 1, \, \omega^{(in)}, \omega^{(ln)}, \omega^{(bn)} \in (0, 1)
$$

и, аналогично,

$$
\psi^{(in)} + \psi^{(ln)} + \psi^{(bn)} = 1, \, \psi^{(in)}, \psi^{(ln)}, \psi^{(bn)} \in (0, 1)
$$

Поэтому будем получать их в виде *softmax* преобразования:

$$
\omega^{(k)} = \frac {\exp(\lambda_k)} {\sum_{j\in\{in, ln, bn\}} \exp(\lambda_j)},\, k\in\{in, ln, bn\}
$$

$$
\psi^{(k)} = \frac {\exp(\delta_k)} {\sum_{j\in\{in, ln, bn\}} \exp(\delta_j)},\, k\in\{in, ln, bn\}
$$

подберая веса $\lambda_k,\,\delta_k$ во время тренирировки.


## Перенос информации

Однако, регуляризация данных и ускорение тренировки нейронных сетей не единственное для чего может быть применен подход с нормализацией.

### Conditional Instance Normalization (CIN)

Выше мы уже отмечали, что IN была предложена в статье о переносе стиля с одного изображения на другое с сохранением контента последнего. В [9] авторы
предлагают улучшение IN, а именно, сохранять для каждого слоя не один набор параметров $\gamma_{c},\,\beta_{c},\,c=0,...,C-1$, а две матрицы размера
$N \times C$ т.е. свой набор для каждого из $N$ изображений стиль которого хотим уметь переносить.

Теперь для переноса стиля, нам достаточно посчитать мат. ожидание и среднее аналогично тому как это делалось в IN и сдвинуть среднее и дисперсию
признаков, а вот параметры афинного преобразования взять от изображения стиль которого хотим наложить:

$$
b_{y,x,c} = \frac {\gamma_{s,c}} {\sigma_{c} + \varepsilon} (a_{y,x,c} - \mu_{c})  + \beta_{s, c}
$$

Интересно, что в статье авторы линейно объединяя два набора параметров от разных стилей переносят на новое изображение некий усредненный стиль.

*Замечание. Я тут описываю процесс стильно упрощенно, про перенос стилей изображений лучше начинать читать с [10] и дальше про использование
нормализации в этой задаче читать [8], [9], [11] и т.д.* 


### Adaptive Instance Normalization (AdIN)

Развитие истории CIN описывается в [11], где вместо того, чтобы предварительно собрать наборы параметров $\gamma,\,\beta$ для разных стилей, эти
параметры производятся "на лету". Т.е. два изображения, одно с контентом $I$, второе с целевым стилем $S$ передаются в одну сеть, и далее в выходах
слоёв нейронной сети статистики первого, заменяют на статистики второго:

$$
b_{n,y,x,c} = \sigma_{c}(S) \left(\frac {a_{y,x,c}(I) - \mu_{c}(I)} {\sigma_{c}(I) + \varepsilon} \right)  + \mu_{c}(S)
$$

*Опять же сильно упрощенно описываю, более подробно структуру сети и AdIN см. в [11]*

### Dynamic Batch Normalization

В статье [12] авторы решают задачу *visual question answering*, т.е. учат сеть отвечать на вопрос заданный по картинке. Для этого они используют
две сети: ResNet для обработки изображения и LSTM, чтобы работать с текстом вопроса. После обработки признаки от изображения и от текста поступают
на заключительный Fusion Block, который генерирует ответ на вопрос.

Авторы предлагают, для каждого BN слоя в ResNet генерировать сдвиги $\Delta\beta$ и $\Delta\gamma$ параметров афинного преобразования, при помощи
полносвязных слоёв, на вход которых подаются признаки сформированные реккурентной сетью, обрабатывающей текст вопроса.

$$
b_{n,y,x,c} = \frac {\gamma_{c} + \Delta\gamma_{c}} {\sigma_{c} + \varepsilon} (a_{n,y,x,c} - \mu_{c})  + \beta_{c} + \Delta \beta_{c}
$$

$$
\Delta\gamma_{c} = MLP_{\gamma}(LSTM(Q)),\,\\
\Delta \beta_{c} = MLP_{\beta}(LSTM(Q)),\,\\
$$

### Positional Normalization (PN)

Следующий вариант нормализации описан в статье [13], в которой авторы работают и генеративными сетями типа энкодер-декодер (например, такими как
CycleGAN или U-Net). Для изображений у нас еще остался один не использованный вариант подсчета моментов, а именно по оси признаков в каждой
пространственной точке тензора:

$$
\mu_{n, y, x} = \frac 1 {C} \sum_{c=0}^{C-1} a_{n, y, x, c}
$$

$$
\sigma_{n, y, x} = \sqrt{\frac 1 {C}\sum_{c=0}^{C-1} (a_{n, y, x, c} - \mu_{n, y, x})^2} 
$$

таким образом, для каждого выхода со сверточного слоя, получается два "черно-белых изображение": мат. ожидание и дисперсия. Эти два тензора авторы
предлагают прокидывать по горизонтальным связям из энкодера в декодер и использовать как параметры афинного преобразования выхода слоя декодера (т.е.
$\mu,\,\sigma$ слоя энкодера использовать в качестве $\beta,\,\gamma$ в соответствующем слое декодера), как бы сдвигая распределение в нужном
направлении. Такой метод передачи статистик авторы называют *Moment Shortcut*.

В качестве естественного обобщения данного метода авторы предлагают *Dynamic Moment Shortcut*, в котором вместо того, чтобы передавать моменты по
горизонтальной связи "как есть", их пропускают через несколько сверточных слоёв, возможно расширяя количество каналов у получающихся в результате
$\beta,\,\gamma$ до количества каналов данных на выходе со слоя декодера.

*Замечание. Можно считать, что для изображений этим способом тоже исчерпаны все возможные варианты подсчета статистик*

## Заключение

Мы рассмотрели шесть вариантов нормализации, которые можно использовать для регуляризации данных проходящих через нейронные сети, и ускорения процесса
тренировки. Если рассуждать о пределах использования, то в задачах общего вида, в свёрточных и полносвязных слоях в основном используется BN, однако,
для рекуррентных сетей лучше подходит LN (хотя, как было отмечено, можно использовать и BN, только надо делать это специфическим образом). Для
генеративных свёрточных сетей работающих с изображениями, лучше подходит IN (для генератора). SN - это скорее нечто экзотичное, в реальных ситуациях
малоприменимое.

Во второй части собраны примеры того, как используя нормализацию, переносить или добавлять информацию к обрабатываемому изображению. Замена мат.
ожидания и дисперсии, позволяет переносить стиль с одного изображения на другое или подмешивать информацию, извлеченную из текста вопроса в
информацию, полученную из изображения. 

PN предлагает интересный способ переносить по горизонтальным связям между энкодером и декодером, не тензор целиком, а только его статистики, получая
при этом хорошие результаты.

### Литература

1. *A. Krizhevsky, I. Sutskever, and G. Hinton, "Imagenet classification with deep convolutional neural networks",
[In NIPS](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), 2012.*

2. *S. Ioffe, Ch. Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
[arXiv:1502.03167v3](https://arxiv.org/abs/1502.03167), 2015*

3. *César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, Yoshua Bengio, "Batch Normalized Recurrent Neural Networks",
[arXiv:1510.01378](https://arxiv.org/abs/1510.01378), 2015*

4. *Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville, "Recurrent Batch Normalization",
[arXiv:1603.09025](https://arxiv.org/abs/1603.09025), 2016*

5. *Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, "Layer Normalization",
[arXiv:1607.06450](https://arxiv.org/abs/1607.06450), 2016*

6. *Yuxin Wu, Kaiming He, "Group Normalization",
[arXiv:1803.08494](https://arxiv.org/abs/1803.08494), 2018*

7. *Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, "Instance Normalization: The Missing Ingredient for Fast Stylization",
[arXiv:1607.08022](https://arxiv.org/abs/1607.08022), 2016*

8. *Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, Jingyu Li, "Differentiable Learning-to-Normalize via Switchable Normalization",
[arXiv:1806.10779](https://arxiv.org/abs/1806.10779), 2018*

9. *Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur, "A Learned Representation For Artistic Style",
[arXiv:1610.07629](https://arxiv.org/abs/1610.07629), 2016*

10. *Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, "A Neural Algorithm of Artistic Style",
[arXiv:1508.06576](https://arxiv.org/abs/1508.06576), 2015*

11. *Xun Huang, Serge Belongie, "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
[arXiv:1703.06868](https://arxiv.org/abs/1703.06868), 2017*

12. *Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, Aaron Courville, "Modulating early visual processing by language",
[arXiv:1707.00683](https://arxiv.org/abs/1707.00683), 2017*

13. *Boyi Li, Felix Wu, Kilian Q. Weinberger, Serge Belongie, "Positional normalization",
[arXiv:1907.04312](https://arxiv.org/abs/1907.04312), 2019*

14. *T. Salimans and D. P. Kingma, "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
[arXiv:1602.07868](https://arxiv.org/pdf/1602.07868.pdf), 2016*
